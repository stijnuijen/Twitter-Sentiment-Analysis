{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules, load data, split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing tweets (the original data file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pprint is 'pretty print', simply a print function that gives 'nicer' outputs than print\n",
    "from pprint import pprint\n",
    "\n",
    "file_test = 'geotagged_tweets_20160812-0912.jsons'\n",
    "tweets_file = open(file_test, \"r\")\n",
    "tweets_data = []\n",
    "\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pandas dataframe with text, language & country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = pd.DataFrame()\n",
    "\n",
    "tweets_test['text'] =    list(map(lambda tweet: tweet['text'], tweets_data))\n",
    "tweets_test['lang'] =    list(map(lambda tweet: tweet['lang'], tweets_data))\n",
    "tweets_test['type'] = list(map(lambda tweet: tweet['place']['place_type'] if tweet['place'] != None else None, tweets_data))\n",
    "tweets_test['location'] = list(map(lambda tweet: tweet['place']['full_name'] if tweet['place'] != None else None, tweets_data))\n",
    "tweets_test['country'] = list(map(lambda tweet: tweet['place']['country'] if tweet['place'] != None else None, tweets_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_test['text']\n",
    "tweets_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test.iloc[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the training file from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = 'kaggletweets.csv'\n",
    "# file_test = 'tweetdata.csv'\n",
    "cols = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "tweets_train = pd.read_csv(file_train, delimiter=',', encoding='latin1', header=None, names=cols)\n",
    "# tweets_test = pd.read_csv(file_test, delimiter=',', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean tweets from hyperlinks and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "tweets_train.text = tweets_train.text.apply(lambda tweet: re.sub(r'https\\S+', '', tweet))\n",
    "tweets_train.text = tweets_train.text.apply(lambda tweet: re.sub(r'http\\S+', '', tweet))\n",
    "tweets_train.text = tweets_train.text.apply(lambda tweet: re.sub(r'@\\S+', '', tweet))\n",
    "\n",
    "# test set\n",
    "tweets_test.text = tweets_test.text.apply(lambda tweet: re.sub(r'https\\S+', '', tweet))\n",
    "tweets_test.text = tweets_test.text.apply(lambda tweet: re.sub(r'http\\S+', '', tweet))\n",
    "# tweets_test.text = tweets_test.text.apply(lambda tweet: re.sub(r'@\\S+', '', tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only want english tweets from us as the tfidfvectorizer will work only with english words\n",
    "tweets_test = tweets_test[(tweets_test.lang == 'en') & (tweets_test.country == 'United States')]\n",
    "tweets_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and test set with train_test_split\n",
    "To assess the quality of the model we split the tweets from the Kaggle into training and test sets as well. THIS IS A DIFFERENT TEST SET THAT IS ONLY USED TO CHECK MODEL PERFORMANCE. When the model is trained we use 'tweets_test' to predict labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets_train.text, tweets_train.target,\n",
    "                                                   train_size=0.1, test_size=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Vectorizer settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words only, no stemming or lemmatization\n",
    "# punctuation is completely ignored\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{2,}',\n",
    "    ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit vectorizer to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer.fit(X_train)\n",
    "# X_train_word_features = word_vectorizer.transform(X_train)\n",
    "# test_features = word_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform test set with fitted vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not too sure if this is necessary as it wont work with predict\n",
    "test_features = word_vectorizer.transform(tweets_test.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Specify classifier and create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "pipe = make_pipeline(word_vectorizer, classifier)\n",
    "param_grid = {'multinomialnb__alpha': [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict labels for the test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predictions = grid.predict(tweets_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = neg, 2 = neutral, 4 = pos\n",
    "tweets_test['target'] = sentiment_predictions\n",
    "tweets_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'tweet_classifications2.csv'\n",
    "tweets_test.to_csv(outfile, sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_table = pd.read_csv('tweet_classifications2.csv')\n",
    "read_table.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
