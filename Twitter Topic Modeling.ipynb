{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import NMF\n",
    "import gensim\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweetdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(tweets.text)\n",
    "tweets['text'] = tweets['text'].str.replace('[','')\n",
    "tweets['text'] = tweets['text'].str.replace(']','')\n",
    "tweets['text'] = tweets['text'].str.replace(\"'\",'')\n",
    "tweets['text'] = tweets['text'].str.replace(\" \",'')\n",
    "tweets['text'] = tweets['text'].str.split(\",\")\n",
    "tweets = tweets['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.add('amp')\n",
    "\n",
    "\n",
    "# Create a set of punctuation words \n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# This is the function makeing the lemmatization\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# In this function we perform the entire cleaning\n",
    "def clean(doc):\n",
    "    stop_free = [i for i in doc if i not in stop]\n",
    "    punc_free = [ch for ch in stop_free if ch not in exclude]\n",
    "    normalized = [lemma.lemmatize(word) for word in punc_free]\n",
    "    return normalized\n",
    "\n",
    "# This is the clean corpus.\n",
    "doc_clean_BOW = [clean(doc) for doc in tweets] \n",
    "doc_clean = [' '.join(x) for x in doc_clean_BOW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer().fit(doc_clean)\n",
    "tf = vect.transform(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.10, min_df=5,\n",
    "                                   max_features=5000,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the NMF model\n",
    "\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(doc_clean_BOW)\n",
    "\n",
    "counter = (dictionary.doc2bow(text) for text in doc_clean_BOW)\n",
    "\n",
    "#LSI\n",
    "lsi = gensim.models.lsimodel.LsiModel(corpus=counter, id2word=dictionary, num_topics=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi.show_topics(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(doc_clean_BOW)\n",
    "\n",
    "counter = (dictionary.doc2bow(text) for text in doc_clean_BOW)\n",
    "\n",
    "#LSI\n",
    "lsi5 = gensim.models.lsimodel.LsiModel(corpus=counter, id2word=dictionary, num_topics=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi5.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=1,\n",
    "                                doc_topic_prior=1,\n",
    "                                learning_method='online',\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = vect.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mexico: \n",
    "que @epn los por para mxico mexicano muro como una\n",
    "\n",
    "Clinton's emails: \n",
    "hillary potus foundation timkaine email billclinton politico crooked lie liar\n",
    "\n",
    "African-Americans always vote democrats: \n",
    "vote black american reason african want win wont democrat voter\n",
    "\n",
    "Trump's refusal to release tax return:\n",
    "tax return release #crookedon #crookeddon vote record like pay #maga\n",
    "\n",
    "Pro-Trump:\n",
    "maga neverhillary trumppence crookedhillary trumptrain makeamericagreatagain americafirst teamtrump demexit hexit\n",
    "\n",
    "Pro-Clinton:\n",
    "imwithher strongertogether hillaryclinton voteblue clintonkaine uniteblue love join tacotrucksoneverycorner posting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics:\n",
    "https://www.bbc.com/news/election-us-2016-37241284 (Mexico)\n",
    "https://www.politico.com/magazine/story/2016/09/hillary-clinton-emails-2016-server-state-department-fbi-214307 (Clinton's emails)\n",
    "https://www.nytimes.com/2016/03/16/opinion/campaign-stops/will-the-democrats-ever-face-an-african-american-revolt.html (African-Americans always vote Democrats)\n",
    "https://www.bbc.com/news/election-us-2016-36382410 (Trump's refusal to release tax returns)\n",
    "Pro-Trump tweets\n",
    "Pro-Clinton tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
